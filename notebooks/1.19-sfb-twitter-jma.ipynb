{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "917ec369-8e95-4736-841b-beacbab4ef7c",
   "metadata": {},
   "source": [
    "# get disaster time series data from twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8076218c-1a00-4d84-9b73-05a062482d7b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## setup connection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b3b914-d401-41f6-b439-c5280cddf2ec",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "714d1e38-1220-4056-88f3-29fdc1cd1db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, json\n",
    "import twitter, re, datetime, pandas as pd\n",
    "from datetime import datetime as dt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "raw",
   "id": "949115e8-de93-4ab4-8724-a9d465af6b86",
   "metadata": {},
   "source": [
    "!{sys.executable} -m pip install twitter python-twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a434e80a-ba39-4910-b3a0-be865d215fa9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### some paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "be8a85be-b173-43e7-aabb-d1a925d613d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdumps_path = '../data/raw/jawiki/dumps_unzipped/'\n",
    "processed_path = '../data/processed/jawiki/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23d0c08-1a94-4a43-b6e8-b4013499cc3c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### define class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "68d3d20b-6bf9-4219-9fab-50ffd50273d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetMiner(object):\n",
    "\n",
    "    result_limit    =   2    \n",
    "    api             =   False\n",
    "    data            =   []\n",
    "    \n",
    "    def __init__(self, keys_dict, api, result_limit = 2):\n",
    "        \n",
    "        self.api = api\n",
    "        self.twitter_keys = keys_dict\n",
    "        \n",
    "        self.result_limit = result_limit\n",
    "        \n",
    "\n",
    "    def mine_user_tweets(self, user=\"jma_bousai\", mine_retweets=True, max_pages=5, last_tweet_id=False):\n",
    "\n",
    "        data           =  []\n",
    "        last_tweet_id  =  last_tweet_id\n",
    "        page           =  1\n",
    "        \n",
    "        while page <= max_pages:\n",
    "            \n",
    "            if last_tweet_id:\n",
    "                statuses   =   self.api.GetUserTimeline(screen_name=user, count=self.result_limit, max_id=last_tweet_id - 1)        \n",
    "            else:\n",
    "                statuses   =   self.api.GetUserTimeline(screen_name=user, count=self.result_limit)\n",
    "                \n",
    "            for item in statuses:\n",
    "\n",
    "                mined = {\n",
    "                    'tweet_id':        item.id,\n",
    "                    'handle':          item.user.name,\n",
    "                    'retweet_count':   item.retweet_count,\n",
    "                    'full_text':            item.text,\n",
    "                    'mined_at':        datetime.datetime.now(),\n",
    "                    'created_at':      item.created_at,\n",
    "                }\n",
    "                \n",
    "                last_tweet_id = item.id\n",
    "                data.append(mined)\n",
    "                \n",
    "            page += 1\n",
    "            print(page)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796359f5-106b-4c3b-ba61-9d6e3908d031",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### get twitter_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b62a8273-93e2-4d12-b67a-a96292a4089c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "spec = importlib.util.spec_from_file_location(\"twitter_keys\", \"../../.twitter_keys.py\")\n",
    "tw_keys = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(tw_keys)\n",
    "twitter_keys = tw_keys.twitter_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82ed6b9-c9a4-4f75-9669-2b1f4dfb342c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### instantiate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "aeeaf569-2d16-490c-ab84-dff89002a65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "api = twitter.Api(\n",
    "    consumer_key         =   twitter_keys['consumer_key'],\n",
    "    consumer_secret      =   twitter_keys['consumer_secret'],\n",
    "    access_token_key     =   twitter_keys['access_token_key'],\n",
    "    access_token_secret  =   twitter_keys['access_token_secret']\n",
    ")\n",
    "\n",
    "miner = TweetMiner(twitter_keys, api, result_limit=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2abfb3b-5b87-4aa0-b2a9-8025ab4ba8ab",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### unused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc846427-31f7-4662-997e-eb7d2f2887d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcc = miner.mine_user_tweets(user='McConnellPress', max_pages=100)\n",
    "aoc = miner.mine_user_tweets(user='aoc', max_pages=100)\n",
    "\n",
    "pon = miner.mine_user_tweets(user='Pontifex', max_pages=100)\n",
    "\n",
    "\n",
    "# ### Convert the tweet ouputs to a pandas DataFrame\n",
    "mcc = pd.DataFrame(mcc)\n",
    "aoc = pd.DataFrame(aoc)\n",
    "pon = pd.DataFrame(pon)\n",
    "\n",
    "mcc['user'] = 'mcc'\n",
    "aoc['user'] = 'aoc'\n",
    "pon['user'] = 'pon'\n",
    "\n",
    "\n",
    "# ##  Create the training data\n",
    "# 1. Mine Trump tweets\n",
    "# - Create a tweet DataFrame\n",
    "# - Mine Sanders tweets\n",
    "# - Append the results to our DataFrame\n",
    "df = pd.concat((aoc[['user', 'text']], \n",
    "                mcc[['user', 'text']], \n",
    "                pon[['user', 'text']])).reset_index(drop=True)\n",
    "\n",
    "X = df.text\n",
    "y = df.user\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "Xtr, Xte, ytr, yte = train_test_split(X,y, test_size=.2)\n",
    "\n",
    "# ## Any interesting ngrams going on with Trump?\n",
    "# ---\n",
    "# \n",
    "# Set up a vectorizer from sklearn and fit the text of Trump's tweets with an ngram range from 2 to 4. Figure out what the most common ngrams are.\n",
    "# \n",
    "# > **Note:** It's up to you whether you want to remove stopwords or not. How does keeping or removing stopwords affect the results?\n",
    "\n",
    "# In[95]:\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "# ### Look at the ngrams for Bernie Sanders\n",
    "\n",
    "# In[96]:\n",
    "\n",
    "\n",
    "ctvec0 = CountVectorizer(ngram_range=(2,2), stop_words='english')\n",
    "\n",
    "\n",
    "# In[97]:\n",
    "\n",
    "\n",
    "ctvec0.fit(Xtr)\n",
    "\n",
    "\n",
    "# In[98]:\n",
    "\n",
    "\n",
    "Xtr_vec0 = ctvec0.transform(Xtr)\n",
    "Xtr_words0 = ctvec0.get_feature_names_out()\n",
    "\n",
    "\n",
    "# In[99]:\n",
    "\n",
    "\n",
    "Xtr_df0 = pd.DataFrame.sparse.from_spmatrix(Xtr_vec0, columns=Xtr_words0)\n",
    "\n",
    "\n",
    "# In[100]:\n",
    "\n",
    "\n",
    "Xtr_df0.sum().sort_values(ascending=False).head(10)\n",
    "\n",
    "\n",
    "# ## Processing the tweets and building a model\n",
    "# \n",
    "# ---\n",
    "# \n",
    "# To do classfication we will need to convert the tweets into a set of features.\n",
    "# \n",
    "# **You will need to:**\n",
    "# - Vectorize input text data.\n",
    "# - Intialize a model (try Logistic regression).\n",
    "# - Train / Predict / cross-validate.\n",
    "# - Evaluate the performance of the model.\n",
    "# \n",
    "# > **Bonus:** you may have noticed that there are website links in the tweets. What additional preprocessing steps can you do before building the model?\n",
    "# \n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "# BONUS\n",
    "# Using the textacy package to do some more comprehensive preprocessing\n",
    "# http://textacy.readthedocs.io/en/latest/\n",
    "\n",
    "# !conda install --yes --prefix {sys.prefix} textacy \n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "# !conda install -c conda-forge textacy -y\n",
    "\n",
    "import textacytextacy.__version__from textacy.preprocessing.normalize import normalize_quotation_marks, normalize_whitespace, normalize_hyphenated_words\n",
    "from textacy.preprocessing.remove import remove_accents, remove_punctuation\n",
    "from textacy.preprocessing.replace import replace_user_handles,replace_currency_symbols,replace_emails,replace_urls,replace_phone_numbers,replace_emojis,replace_numbers\n",
    "# In[101]:\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "from collections import Counter\n",
    "\n",
    "# We can use the TfidfVectorizer to find ngrams for us\n",
    "#vect = TfidfVectorizer(ngram_range=(2,4),stop_words=text.ENGLISH_STOP_WORDS.union(['https','co']))\n",
    "vect = TfidfVectorizer(ngram_range=(2,4),stop_words=text.ENGLISH_STOP_WORDS)\n",
    "\n",
    "# Pulls all of Scottie's tweet text's into one giant string\n",
    "summaries = \"\".join(df['text'])\n",
    "ngrams_summaries = vect.build_analyzer()(summaries)\n",
    "\n",
    "Counter(ngrams_summaries).most_common(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932faf0d-7703-4716-9b1d-a76545e512a9",
   "metadata": {},
   "source": [
    "## get data from @jma_bousai"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7ae87ecc-eea3-4ef9-b6e2-dd4228d64934",
   "metadata": {},
   "source": [
    "jma_bousai = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c45c9c-cf16-4b7c-a0f6-4238eeb8db5d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### raw results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e0245c-518c-4539-98e1-032a72ae1ca0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### result 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "4115e7b3-4dc4-4ee8-98d1-3daff8e348ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "217"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[('Fri Oct 04 05:06:19 +0000 2019', 1179986079811719168),\n",
       " ('Sat Oct 05 00:28:18 +0000 2019', 1180278502618042368),\n",
       " ('Sat Oct 05 00:48:05 +0000 2019', 1180283482494259200),\n",
       " ('Sun Oct 06 00:26:56 +0000 2019', 1180640546353643521)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(\n",
    "    len(jma_bousai[x]),\n",
    "    [(i['created_at'], i['tweet_id']) for i in jma_bousai[x][-1:-5:-1]]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06edc86a-08b8-490c-b040-09c7bd73f868",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### actual result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "b270441b-3c6c-49b5-9111-eb8a20329867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "x=2\n",
    "jma_bousai[x] = miner.mine_user_tweets(user='jma_bousai', max_pages=20, last_tweet_id=1278950916482195456)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4caaf287-f1cb-4440-9b30-b04cea9d3a5c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### result 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "b4fb24b5-a4e9-488a-9eeb-4ec8dc31765e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[('Fri Jul 03 07:17:14 +0000 2020', 1278950916482195456),\n",
       " ('Fri Jul 03 07:17:43 +0000 2020', 1278951035315171329),\n",
       " ('Fri Jul 03 07:18:15 +0000 2020', 1278951169583312906),\n",
       " ('Fri Jul 03 07:18:44 +0000 2020', 1278951291410984960)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(\n",
    "    len(jma_bousai[x]),\n",
    "    [(i['created_at'], i['tweet_id']) for i in jma_bousai[x][-1:-5:-1]]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b65c67-0637-4a0f-9693-c6b9a2693591",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### actual result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "eef7fa45-7bf6-472f-8298-b049421275f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "x=1\n",
    "jma_bousai[x] = miner.mine_user_tweets(user='jma_bousai', max_pages=20, last_tweet_id=1426241703619403779)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26233212-6e57-443c-a57f-7443ed5e1b7a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### result 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "fe95b556-1ccd-4949-b937-ef70b16c6d3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[('Fri Aug 13 17:58:15 +0000 2021', 1426241703619403779),\n",
       " ('Fri Aug 13 18:43:39 +0000 2021', 1426253130807742464),\n",
       " ('Fri Aug 13 20:52:06 +0000 2021', 1426285454203121664),\n",
       " ('Fri Aug 13 21:13:30 +0000 2021', 1426290840901586946)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(\n",
    "    len(jma_bousai[x]),\n",
    "    [(i['created_at'], i['tweet_id']) for i in jma_bousai[x][-1:-5:-1]]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14cb16f-b76d-454f-9908-c086099b0efd",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### actual result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "f7a0cdf3-fc59-44fc-a71c-9ab05c7b7223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "x=0\n",
    "jma_bousai[0] = miner.mine_user_tweets(user='jma_bousai', max_pages=20, last_tweet_id=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542d7b5e-433f-448f-aef8-f220cc6124e3",
   "metadata": {},
   "source": [
    "### clean tweets to df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3877e54f-05f8-4d37-9119-410beee4c611",
   "metadata": {},
   "source": [
    "#### make dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "b12f053d-7950-495e-b058-f116015d4488",
   "metadata": {},
   "outputs": [],
   "source": [
    "bousai = []\n",
    "for i in jma_bousai:\n",
    "    bousai = bousai + jma_bousai[i]\n",
    "bousai = pd.DataFrame.from_records(bousai)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f09689-67e4-40f5-ad32-7c152adbf24f",
   "metadata": {},
   "source": [
    "#### extract headline categories from fulltext as dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "5107dadb-17c3-4f9b-be54-1beeb3207388",
   "metadata": {},
   "outputs": [],
   "source": [
    "bousai = (bousai\n",
    "    .assign(火_火山=lambda x: x.full_text.str.count(r'火山'))\n",
    "    .assign(火_噴火警戒レベル１=lambda x: x.full_text.str.count(r'噴火警戒レベル１'))\n",
    "    .assign(火_火口周辺規制=lambda x: x.full_text.str.count(r'火口周辺規制'))\n",
    "    .assign(火_入山規制=lambda x: x.full_text.str.count(r'入山規制'))\n",
    "      \n",
    "    .assign(雨_警戒レベル５=lambda x: x.full_text.str.count(r'警戒レベル５'))\n",
    "    .assign(雨_大雨=lambda x: x.full_text.str.count(r'大雨'))\n",
    "    .assign(雨_土砂災害=lambda x: x.full_text.str.count(r'土砂災害'))\n",
    "\n",
    "    .assign(震_地震=lambda x: x.full_text.str.count(r'地震'))\n",
    "    .assign(震_震度5=lambda x: x.full_text.str.count(r'震度5'))\n",
    "    .assign(震_震度678=lambda x: x.full_text.str.count(r'震度6|震度7|震度8'))\n",
    "      \n",
    "    .assign(雪_大雪=lambda x: x.full_text.str.count(r'大雪'))\n",
    "    .assign(雪_路面凍結=lambda x: x.full_text.str.count(r'路面凍結'))\n",
    "    \n",
    "    .assign(津_津波=lambda x: x.full_text.str.count(r'津波'))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8dcf6b-3dd5-4cd5-bed9-f09d9c3800de",
   "metadata": {},
   "source": [
    "#### string to timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "4ae827dc-b5c5-4357-96cc-4fe2122dd4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_datetime(old_dt):\n",
    "    return dt.strftime(dt.strptime(old_dt,'%a %b %d %H:%M:%S +0000 %Y'), '%Y-%m-%d %H:%M:%S')\n",
    "# https://stackoverflow.com/questions/7703865/going-from-twitter-date-to-python-datetime-date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "8478835b-a9c2-46c7-9715-852a00b24030",
   "metadata": {},
   "outputs": [],
   "source": [
    "bousai['dt_created'] = pd.to_datetime(bousai.created_at)\n",
    "bousai = bousai.set_index('dt_created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "d05fb6dc-f17d-463d-b357-ef66cb539373",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(processed_path + 'jma_bousai.pickle', 'wb') as f:\n",
    "    pickle.dump(bousai, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e85d172-32d9-4aee-a8a8-e62f82907050",
   "metadata": {},
   "source": [
    "## END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
