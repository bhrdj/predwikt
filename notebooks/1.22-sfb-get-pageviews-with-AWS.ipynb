{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48660241-8117-4a73-b0b4-df969997b4ec",
   "metadata": {},
   "source": [
    "# get pageviews with AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19abd44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mysql_user = 'ubuntu'\n",
    "# mysql_pass = input(f'Enter the MySQL password for user {mysql_user}: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efed6e0",
   "metadata": {},
   "source": [
    "#### how to do jupyter with aws"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b3ed43",
   "metadata": {},
   "source": [
    "- https://dataschool.com/data-modeling-101/running-jupyter-notebook-on-an-ec2-server/\n",
    "    - except jupyter_notebook_config.py_ should be ...config.py instead\n",
    "- https://gist.github.com/J535D165/0e840291e7b2598ec157e13e9b9ca569\n",
    "    - trying this for how to use nohup\n",
    "- some medium [article](https://medium.com/@christinakouride/a-beginners-guide-to-running-jupyter-notebook-on-amazon-ec2-69e1b74e73cc#:~:text=Your%20Jupyter%20Notebook%20server%20will,for%20time%20not%20using%20it.)\n",
    "    - they pointed out that notebook will keep running, but didn't mention nohup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17eba2f0-f33b-40f6-ae44-30c348ca5a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, requests, gzip, pickle, io, logging, inspect, functools\n",
    "import pandas as pd, datetime as dt\n",
    "import mysql.connector as mysql, sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42c9239c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.7\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18fa73d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "todays_date = dt.datetime.now(tz=dt.timezone(dt.timedelta(hours=8))).strftime('%Y-%m-%d')\n",
    "logfilepath = '../data/logs/pageviews-log_' + todays_date + '_' + str(0) + '.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e907f075",
   "metadata": {},
   "source": [
    "## setup logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcfab96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger('1.22-sfb-get-pageviews-with-AWS')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "# create file handler which logs even debug messages\n",
    "fh = logging.FileHandler(logfilepath)\n",
    "fh.setLevel(logging.DEBUG)\n",
    "# create console handler with a higher log level\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.ERROR)\n",
    "# create formatter and add it to the handlers\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "fh.setFormatter(formatter)\n",
    "ch.setFormatter(formatter)\n",
    "# add the handlers to the logger\n",
    "logger.addHandler(fh)\n",
    "logger.addHandler(ch)\n",
    "\n",
    "# USAGE:\n",
    "# logger.info('foobarbazquxquuxquuzcorgegraultgarplywaldofredplughxyzzythud')\n",
    "\n",
    "# https://docs.python.org/3/howto/logging-cookbook.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83902168",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_args(func):\n",
    "    \"\"\"\n",
    "    Decorator to print function call details.\n",
    "    This includes parameters names and effective values.\n",
    "    \"\"\"\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        func_args = inspect.signature(func).bind(*args, **kwargs).arguments\n",
    "        func_args_str = \", \".join(map(\"{0[0]} = {0[1]!r}\".format, func_args.items()))\n",
    "        logger.info(f\"start {func.__module__}.{func.__qualname__} ( {func_args_str} )\")\n",
    "        try:\n",
    "            return func(*args, **kwargs)\n",
    "        finally:\n",
    "            logger.info(f\"finish {func.__module__}.{func.__qualname__} ( {func_args_str} )\\n\")\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f3bab91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_simply(func):\n",
    "    \"\"\"\n",
    "    Decorator to print function call details.\n",
    "    This includes parameters names and effective values.\n",
    "    \"\"\"\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        func_args = inspect.signature(func).bind(*args, **kwargs).arguments\n",
    "        func_args_str = \", \".join(map(\"{0[0]} = {0[1]!r}\".format, func_args.items()))\n",
    "        logger.info(f\"start {func.__module__}.{func.__qualname__}\")\n",
    "        try:\n",
    "            return func(*args, **kwargs)\n",
    "        finally:\n",
    "            logger.info(f\"finish {func.__module__}.{func.__qualname__}\\n\")\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05b0f06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_errors(func):\n",
    "    \"\"\"\n",
    "    A decorator that wraps the passed in function and logs \n",
    "    exceptions should one occur\n",
    "    \"\"\"\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        try:\n",
    "            return func(*args, **kwargs)\n",
    "        except:\n",
    "            # log the exception\n",
    "            err = \"There was an exception in  \"\n",
    "            err += func.__name__\n",
    "            logger.exception(err)\n",
    "\n",
    "            # re-raise the exception\n",
    "            raise\n",
    "    return wrapper\n",
    "# https://www.blog.pythonlibrary.org/2016/06/09/python-how-to-create-an-exception-logging-decorator/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4293d3",
   "metadata": {},
   "source": [
    "##### login to mariadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bfb36a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_simply\n",
    "def connect_mariadb(host='localhost', user=mysql_user, passwd=mysql_pass, dbname='jawiki'):\n",
    "    \"\"\"\n",
    "    connect to mariadb and return: \n",
    "        cxn, cur, engine, conn\n",
    "    \"\"\"\n",
    "    cxn = mysql.connect(host=host,user=user,passwd=passwd, database=dbname)\n",
    "    cur = cxn.cursor()\n",
    "\n",
    "    connection_str = 'mysql+mysqlconnector://'+user+':'+passwd+'@'+host+'/'+dbname  # removed this after host +':'+dbport\n",
    "    try:\n",
    "        engine = sqlalchemy.create_engine(connection_str)\n",
    "        conn = engine.connect()\n",
    "    except Exception as e:\n",
    "        print('Database connection error - check creds')\n",
    "        print(e)\n",
    "    return cxn, cur, engine, conn\n",
    "\n",
    "cxn, cur, engine, conn = connect_mariadb()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2abe8c",
   "metadata": {},
   "source": [
    "## get page_titles and urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee994dc",
   "metadata": {},
   "source": [
    "## functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af52ca3",
   "metadata": {},
   "source": [
    "###### function get_pageviews_urls_and_outpaths_by_years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a18b6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_args\n",
    "def get_pageviews_urls_by_year(year:int) -> list[str]:\n",
    "    \"\"\"\n",
    "    Programmatically generate the urls and local file structure to get pageviews hourfiles.\n",
    "    INPUT: year as integer\n",
    "    OUTPUT: list of urls to all the hourfiles for that year\n",
    "    NOTE: filenames are formatted as: \n",
    "        'pageviews-20210101-000000.gz', i.e.\n",
    "        'pageviews-yyyymmdd-hhmmss.gz', although 'mmss' is always '0000'    \n",
    "    \"\"\"\n",
    "    hour_strings = list(map(lambda x: str(x).zfill(2), range(0,24)))  # '00' through '23' \n",
    "    dates = pd.date_range(dt.datetime(year,1,1), \n",
    "                          end=dt.datetime(year+1,1,1), \n",
    "                          freq='D')  # , tz='Japan'                   # each date in the year\n",
    "    base_url = 'https://dumps.wikimedia.org/other/pageviews/'\n",
    "    urls = [f'{base_url}{d.year}/{d.year}-{str(d.month).zfill(2)}'\n",
    "            f'/pageviews-{d.year}{str(d.month).zfill(2)}{str(d.day).zfill(2)}-{h}0000.gz'\n",
    "            for d in dates for h in hour_strings]\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899ddcee-6006-4aa1-9219-877a34704748",
   "metadata": {},
   "source": [
    "##### function download_file to temporarily store hourfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1d5b0f9-df90-4710-8349-543a9723c17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_args\n",
    "def download_file(url, dirpath='./'):\n",
    "    \"\"\"\n",
    "    Carefully downloads the file from the url to the local directory dirpath.\n",
    "    Should work for large files, and hopefully for poor connections, etc.\n",
    "    \"\"\"\n",
    "    local_filepath = dirpath + url.split('/')[-1]\n",
    "    # NOTE the stream=True parameter below\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(local_filepath, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=131072):   #8192\n",
    "                # If you have chunk encoded response uncomment if\n",
    "                # and set chunk_size parameter to None.\n",
    "                #if chunk: \n",
    "                f.write(chunk)\n",
    "    return local_filepath\n",
    "# https://stackoverflow.com/questions/16694907/download-large-file-in-python-with-requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37b506d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_args\n",
    "def fread_from_url(url):\n",
    "    \"\"\"\n",
    "    Simply reads the file from the url as txt\n",
    "    For smaller files only (100mb maybe?)\n",
    "    \"\"\"\n",
    "    response = requests.get(target_url)\n",
    "    return response.text\n",
    "# https://stackoverflow.com/questions/16694907/download-large-file-in-python-with-requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc12836f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8cc5b14",
   "metadata": {},
   "source": [
    "###### function get_pageviews_subset_by_project to get all jawiki pageviews from an hourfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42d1fe03",
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_args\n",
    "def open_hour_file(path:str):\n",
    "    \"\"\"\n",
    "    INPUT: path to a .gz file \n",
    "    OUTPUT: open file handle\n",
    "    USAGE:\n",
    "        with open_hour_file(foo) as bar:\n",
    "            baz\n",
    "    BEHAVIOR: if filename doesn't have .gz extension, tries to open without decompression\n",
    "    \n",
    "    \"\"\"\n",
    "    if path[-3:] == \".gz\":\n",
    "        return gzip.open(path, mode=\"rt\", encoding=\"utf-8\", errors=\"replace\")\n",
    "    else:\n",
    "        return open(path, mode=\"rt\", encoding=\"utf-8\", errors=\"replace\")\n",
    "# https://github.com/mediawiki-utilities/python-mwviews/blob/main/src/mwviews/utilities/aggregate.py\n",
    "# https://stackoverflow.com/questions/30582162/creating-a-missing-directory-file-structure-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "186351ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_args\n",
    "def parse_utc_time_from_filename(filename:str) -> dt.datetime:\n",
    "    \"\"\"\n",
    "    filenames are formatted as: \n",
    "        'pageviews-20210101-000000.gz', i.e.\n",
    "        'pageviews-yyyymmdd-hhmmss.gz', although 'mmss' is always '0000'    \n",
    "    \"\"\"\n",
    "    [d, t] = filename.split('-')[1:]  # split filename into yyyymmdd and hhmmss\n",
    "    return dt.datetime(int(d[:4]), int(d[4:6]), int(d[6:]), int(t[:2]), tzinfo=dt.timezone.utc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e599381",
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_args\n",
    "def get_pageviews_subset_by_project(fpath_in, proj:str='ja') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "        fpath: local filepath to pageviews records as text file\n",
    "        proj: mediawiki project code\n",
    "    OUTPUT:\n",
    "        dataframe ready for database ingestion\n",
    "    \"\"\"\n",
    "    lst = []\n",
    "    colnames=['domain_code','page_title','count_views','total_response_size']\n",
    "    # subset pageview records by project as list of strings\n",
    "    with open_hour_file(fpath_in) as f_in:\n",
    "        while (line := f_in.readline()):\n",
    "            if line[:3] != proj + ' ':\n",
    "                continue\n",
    "            lst.append(line)\n",
    "    # turn list of strings into dataframe\n",
    "    df = pd.read_csv(\n",
    "        io.StringIO('\\n'.join(lst))\n",
    "        ,delim_whitespace=True\n",
    "        ,names=colnames)\n",
    "    # add datetime_viewed_UTC as column based on filename\n",
    "    parsed_datetime = parse_utc_time_from_filename(fpath_in.split(sep='/')[-1])\n",
    "    df = df.assign(datetime_viewed_UTC=parsed_datetime)\n",
    "    # add datetime_added_UTC as the time when this function runs\n",
    "    now_datetime = dt.datetime.now(tz=dt.timezone.utc)\n",
    "    df = df.assign(datetime_added_UTC=now_datetime)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f514dfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_args\n",
    "def get_pageviews_subset_by_proj_and_pagetitles(fpath_in, proj:str='ja', pagetitles:list=[]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "        fpath: local filepath to pageviews records as text file\n",
    "        proj: mediawiki project code\n",
    "        pagetitles: whitelist of page_titles \n",
    "    OUTPUT:\n",
    "        dataframe ready for database ingestion\n",
    "    \"\"\"\n",
    "    lst = []\n",
    "    colnames=['domain_code','page_title','count_views','total_response_size']\n",
    "    # subset pageview records by project as list of strings\n",
    "    with open_hour_file(fpath_in) as f_in:\n",
    "        while (line := f_in.readline()):\n",
    "            if line[:3] != proj + ' ':\n",
    "                continue\n",
    "            if line.split(sep=' ')[1] in pagetitles:\n",
    "                lst.append(line)\n",
    "    # turn list of strings into dataframe\n",
    "    df = pd.read_csv(\n",
    "        io.StringIO('\\n'.join(lst))\n",
    "        ,delim_whitespace=True\n",
    "        ,names=colnames)\n",
    "    # add datetime_viewed_UTC as column based on filename\n",
    "    parsed_datetime = parse_utc_time_from_filename(fpath_in.split(sep='/')[-1])\n",
    "    df = df.assign(datetime_viewed_UTC=parsed_datetime)\n",
    "    # add datetime_added_UTC as the time when this function runs\n",
    "    now_datetime = dt.datetime.now(tz=dt.timezone.utc)\n",
    "    df = df.assign(datetime_added_UTC=now_datetime)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b5a868",
   "metadata": {},
   "source": [
    "###### function etl_pageviews_by_years_and_projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cfa77329",
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_args\n",
    "def does_table_exist(tablename:str, dbname:str='jawiki', con=conn) -> bool:\n",
    "    \"\"\"\n",
    "    run SQL query to look for tablename, return boolean\n",
    "    \"\"\"\n",
    "    sql = f\"\"\"\n",
    "    SELECT * \n",
    "        FROM information_schema.tables\n",
    "    WHERE table_schema = '{dbname}' \n",
    "        AND table_name = '{tablename}'\n",
    "    LIMIT 1;\n",
    "    \"\"\"\n",
    "    return bool(pd.read_sql(sql, con).shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e78b562d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_args\n",
    "def create_pageviews_table(con=conn):\n",
    "    \"\"\"\n",
    "    run SQL code to create pageviews table\n",
    "    \"\"\"\n",
    "    \n",
    "    sql = \"\"\"\n",
    "    CREATE TABLE pageviews (\n",
    "        row_id BIGINT(20) AUTO_INCREMENT PRIMARY KEY\n",
    "        ,domain_code TEXT\n",
    "        ,page_title TEXT\n",
    "        ,count_views BIGINT(20)\n",
    "        ,total_response_size BIGINT(20)\n",
    "        ,datetime_viewed_UTC TIMESTAMP DEFAULT 0\n",
    "        ,datetime_added_UTC TIMESTAMP DEFAULT CURRENT_TIMESTAMP \n",
    "    )\n",
    "    ;\n",
    "    \"\"\"\n",
    "    conn.execute(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ae3366",
   "metadata": {},
   "source": [
    "###### calculate finish time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c86e874",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_forecast_of_completion(count, process_start_time, years):\n",
    "    hours_in_year = 365.25*24\n",
    "    num_years = len(years)\n",
    "    frac_done = count / hours_in_year / num_years\n",
    "    time_now = dt.datetime.now()\n",
    "    process_time_so_far = time_now - process_start_time\n",
    "    finish_time = process_start_time + (process_time_so_far / frac_done)\n",
    "    logger.info(\n",
    "        f'-----------\\n'\n",
    "        f'PROCESS TIME SO FAR:  {time_now - process_start_time}\\n'\n",
    "        f'COUNT:                {count}\\n'\n",
    "        f'FRACTION DONE:        {frac_done}\\n'\n",
    "        f'EXPECTED FINISH TIME: {finish_time}\\n'\n",
    "        '-----------\\n'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c548c680",
   "metadata": {},
   "source": [
    "###### unpickle list of pagetitles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96ec40d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def failed_decode(a):\n",
    "    try:\n",
    "        a.decode('utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def bytearray_to_str(a:bytearray, encoding='utf-8') -> str:\n",
    "    if type(a) != bytearray:\n",
    "        return a        \n",
    "    while failed_decode(a):\n",
    "        a = a[:-1]\n",
    "    return str(a.decode(encoding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "20a3529f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/processed/jawiki/' + 'disaster_descendants_raw.pickle', 'rb') as f:\n",
    "    disaster_descendants_raw = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1653c500",
   "metadata": {},
   "outputs": [],
   "source": [
    "disaster_cat_page_ids = {'火山災害':2390743, '熱帯低気圧':626482, '雪害':2390774, '地震':135264, '津波':765772}  # '自然災害':137069, \n",
    "disasters_english = {'火山災害':'VolcanicDisaster', '熱帯低気圧':'TropicalCyclones', '雪害':'SnowDamage', '地震':'Earthquake', '津波':'Tsunami'}\n",
    "disasters = list(disaster_cat_page_ids.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "683358f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\n",
    "for i in disaster_descendants_raw:\n",
    "    d[i] = (disaster_descendants_raw[i]\n",
    "            .drop_duplicates(subset='id')\n",
    "            .applymap(bytearray_to_str)\n",
    "           )\n",
    "    d[i] = d[i][d[i].namespace == 0]\n",
    "    d[i]['page_title'] = d[i].name.map(lambda x: str(x).split(sep='\\n')[-1])\n",
    "disaster_descendants = d\n",
    "del d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "836e9c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_titles = [j for i in disaster_descendants for j in disaster_descendants[i].page_title]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cbae30",
   "metadata": {},
   "source": [
    "## main program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ad5fbe64",
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_errors\n",
    "def etl_pageviews_by_years_and_projects(years:list[int], \n",
    "                                        pagetitles:list[str],\n",
    "                                        project:str='ja', \n",
    "                                        logfilepath:str=logfilepath, \n",
    "                                        con=conn):\n",
    "    if not does_table_exist('pageviews'):\n",
    "        create_pageviews_table(con)\n",
    "    # start counts\n",
    "    process_start_time = dt.datetime.now()\n",
    "    file_count = 0\n",
    "    for year in years:\n",
    "        urls = get_pageviews_urls_by_year(year)\n",
    "        # urls = urls[:3] # truncate if debugging \n",
    "        for url in urls:\n",
    "            \n",
    "            # actual work\n",
    "            temp_fpath = download_file(url, dirpath='../data/temp/')\n",
    "            pageviews = get_pageviews_subset_by_proj_and_pagetitles(\n",
    "                            temp_fpath, pagetitles=pagetitles)\n",
    "            pageviews.to_sql(name='pageviews',con=con, if_exists='append', index=False)\n",
    "            \n",
    "            # complete counts\n",
    "            file_count += 1\n",
    "            # log and cleanup\n",
    "            log_forecast_of_completion(file_count, process_start_time, years)\n",
    "            os.remove(temp_fpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692e85a3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e22fabd",
   "metadata": {},
   "source": [
    "run main program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3c31aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "etl_pageviews_by_years_and_projects([2016,2017,2018,2019,2020,2021], all_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765d2dc3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21641bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql('select count(*) from pageviews;', con=conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edc26a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql('desc pageviews;', con=conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d0ac799a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tables_in_jawiki</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Tables_in_jawiki]\n",
       "Index: []"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_sql('show tables;', con=conn)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8d98b0b1",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "pd.read_sql('select * from pageviews order by datetime_viewed_UTC desc limit 5;', con=conn, index_col='row_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f68421",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0f5730",
   "metadata": {},
   "source": [
    "```sql\n",
    "set password for ubuntu@localhost = PASSWORD('mariadb394')\n",
    "show grants for ubuntu@localhost;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d091082",
   "metadata": {},
   "source": [
    "#### run this in root console\n",
    "```sql\n",
    "set global net_buffer_length=1000000; \n",
    "set global max_allowed_packet=1000000000;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344c3d50",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "raw",
   "id": "75550829",
   "metadata": {},
   "source": [
    "# nohup jupyter notebook &"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a4212a64",
   "metadata": {},
   "source": [
    "# !jupyter nbconvert --to script '1.22-sfb-get-pageviews-with-AWS-lambda.ipynb'"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3d2de286",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1639844a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bc2462",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f348120",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858e7da3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec38d37",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43377fb5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4414d09",
   "metadata": {},
   "source": [
    "#### run this in root console\n",
    "```sql\n",
    "set global net_buffer_length=1000000; \n",
    "set global max_allowed_packet=1000000000;\n",
    "```"
   ]
  },
  {
   "cell_type": "raw",
   "id": "71fe2e66",
   "metadata": {},
   "source": [
    "##### test running cell while laptop sleeping"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9232cf00",
   "metadata": {},
   "source": [
    "###### idea"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cfc50209",
   "metadata": {},
   "source": [
    "- consider the case where:\n",
    "    - i start the following cell's script as follows:\n",
    "        - on AWS from a ssh terminal \n",
    "        - via nohup jupyter on computer A\n",
    "    - then i disconnect the ssh from computer A\n",
    "        - so that the cell is still running in the instance\n",
    "- then i find the result that:\n",
    "    - i can reopen jupyter from another ssh terminal\n",
    "    - i can interrupt the script\n",
    "    - but i can't see its in-process results\n",
    "- strategy for now:\n",
    "    - develop the script on whatever computers\n",
    "    - then run it from a browser tab in my android phone\n",
    "        - that way i can keep \"tabs\" on it (😉)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a7a617f3",
   "metadata": {},
   "source": [
    "###### function ```subset_pageviews_by_page_titles``` to clean and filter pageview records"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e592660a",
   "metadata": {},
   "source": [
    "def subset_pageviews_by_page_titles(fpath_in, fpath_out, page_titles, proj='ja'):\n",
    "    os.makedirs(os.path.dirname(fpath_out), exist_ok=True)\n",
    "    with (open_hour_file(fpath_in) as f_in, \n",
    "          open(fpath_out, 'w', encoding='utf-8') as f_out\n",
    "         ):\n",
    "        while (line := f_in.readline()):\n",
    "            if line[:3] != proj + ' ':\n",
    "                continue\n",
    "            lst = line.strip().split(sep=' ')\n",
    "            if lst[1] in page_titles:\n",
    "                f_out.write(line)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "57484a87",
   "metadata": {},
   "source": [
    "###### function ```subset_pageviews_by_project``` to clean and filter pageview records"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7e560dad",
   "metadata": {},
   "source": [
    "def subset_pageviews_by_project(fpath_in, fpath_out, proj='ja'):\n",
    "    os.makedirs(os.path.dirname(fpath_out), exist_ok=True)\n",
    "    with (open_hour_file(fpath_in) as f_in, \n",
    "          open(fpath_out, 'w', encoding='utf-8') as f_out\n",
    "         ):\n",
    "        while (line := f_in.readline()):\n",
    "            if line[:3] != proj + ' ':\n",
    "                continue\n",
    "            f_out.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7202ba2d",
   "metadata": {},
   "source": [
    "###### FOUND OUT THAT THERE'RE SOME UNRELATED COLUMNS IN THE CATEGORIES:\n",
    "NEED TO CLEAN THE CATEGORIES"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bb5fb98b",
   "metadata": {},
   "source": [
    "for i in disaster_descendants:\n",
    "    print(i)\n",
    "    print(disaster_descendants[i][disaster_descendants[i].page_title.isin(['WHITEBERRY'])])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0ebf2072",
   "metadata": {},
   "source": [
    "disaster_descendants['火山災害'][disaster_descendants['火山災害'].page_title.isin(['WHITEBERRY'])]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ac73caff",
   "metadata": {},
   "source": [
    "all_titles[5444:5470]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6ea46b76",
   "metadata": {},
   "source": [
    "[i for i in all_titles if i == 'WHITEBERRY']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
